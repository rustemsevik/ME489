@article{IKOTUN2023178,
title = {K-means clustering algorithms: A comprehensive review, variants analysis, and advances in the era of big data},
journal = {Information Sciences},
volume = {622},
pages = {178-210},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.11.139},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522014633},
author = {Abiodun M. Ikotun and Absalom E. Ezugwu and Laith Abualigah and Belal Abuhaija and Jia Heming},
keywords = {K-means, K-means variants, Clustering algorithm, Modified k-means, Improved k-means, Perspectives on big data clustering, Big data clustering},
abstract = {Advances in recent techniques for scientific data collection in the era of big data allow for the systematic accumulation of large quantities of data at various data-capturing sites. Similarly, exponential growth in the development of different data analysis approaches has been reported in the literature, amongst which the K-means algorithm remains the most popular and straightforward clustering algorithm. The broad applicability of the algorithm in many clustering application areas can be attributed to its implementation simplicity and low computational complexity. However, the K-means algorithm has many challenges that negatively affect its clustering performance. In the algorithm’s initialization process, users must specify the number of clusters in a given dataset apriori while the initial cluster centers are randomly selected. Furthermore, the algorithm's performance is susceptible to the selection of this initial cluster and for large datasets, determining the optimal number of clusters to start with becomes complex and is a very challenging task. Moreover, the random selection of the initial cluster centers sometimes results in minimal local convergence due to its greedy nature. A further limitation is that certain data object features are used in determining their similarity by using the Euclidean distance metric as a similarity measure, but this limits the algorithm’s robustness in detecting other cluster shapes and poses a great challenge in detecting overlapping clusters. Many research efforts have been conducted and reported in literature with regard to improving the K-means algorithm’s performance and robustness. The current work presents an overview and taxonomy of the K-means clustering algorithm and its variants. The history of the K-means, current trends, open issues and challenges, and recommended future research perspectives are also discussed.}
}
@article{MINH2022109189,
title = {A new metaheuristic optimization based on K-means clustering algorithm and its application to structural damage identification},
journal = {Knowledge-Based Systems},
volume = {251},
pages = {109189},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.109189},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122005913},
author = {Hoang-Le Minh and Thanh Sang-To and Magd {Abdel Wahab} and Thanh Cuong-Le},
keywords = {Metaheuristic optimization, K-means clustering algorithm, Engineering problems, Structural damage identification, SAP2000-OAPI},
abstract = {This paper develops a new metaheuristic optimization algorithm named K-means Optimizer (KO) to solve a wide range of optimization problems from numerical functions to real-design challenges. First, the centroid vectors of clustering regions are established at each iteration using K-means algorithm, then KO proposes two movement strategies to create a balance between the ability of exploitation and exploration. The decision on the movement strategy for exploration or exploitation at each iteration depends on a parameter that will be designed to recognize if each search agent is too long in the region visited with no self-improvement. To demonstrate the effectiveness and reliability of KO, twenty-three classical benchmark functions, CEC2005 and CEC2014 benchmark functions, are employed as a first example and compared with other algorithms. Then, three well-known engineering problems are also considered and their results are compared to the results obtained by the other algorithms. Finally, KO is applied to structural damage identification (SDI) problem of a complex 3D concrete structure including seven stories building having a 25.2 m total height. For this purpose, SAP2000 is used to solve the finite element (FE) model of this structure. Then, for the first time, we successfully developed a sub-program that allows two-way data exchange between SAP2000 and MATLAB through the Open Application Programming Interface (OAPI) library to update the FE model. From the results, we found that KO has the best performance for the considered benchmark functions based on the Wilcoxon rank-sum test and Friedman ranking test. The results obtained in this work have proved the effectiveness and reliability of KO in solving optimization problems, especially for SDI. Source codes of KO is publicly available at http://goldensolutionrs.com/codes.html.}
}
@article{PENA19991027,
title = {An empirical comparison of four initialization methods for the K-Means algorithm},
journal = {Pattern Recognition Letters},
volume = {20},
number = {10},
pages = {1027-1040},
year = {1999},
issn = {0167-8655},
doi = {https://doi.org/10.1016/S0167-8655(99)00069-0},
url = {https://www.sciencedirect.com/science/article/pii/S0167865599000690},
author = {J.M Peña and J.A Lozano and P Larrañaga},
keywords = {-Means algorithm, -Means initialization, Partitional clustering, Genetic algorithms},
abstract = {In this paper, we aim to compare empirically four initialization methods for the K-Means algorithm: random, Forgy, MacQueen and Kaufman. Although this algorithm is known for its robustness, it is widely reported in the literature that its performance depends upon two key points: initial clustering and instance order. We conduct a series of experiments to draw up (in terms of mean, maximum, minimum and standard deviation) the probability distribution of the square-error values of the final clusters returned by the K-Means algorithm independently on any initial clustering and on any instance order when each of the four initialization methods is used. The results of our experiments illustrate that the random and the Kaufman initialization methods outperform the rest of the compared methods as they make the K-Means more effective and more independent on initial clustering and on instance order. In addition, we compare the convergence speed of the K-Means algorithm when using each of the four initialization methods. Our results suggest that the Kaufman initialization method induces to the K-Means algorithm a more desirable behaviour with respect to the convergence speed than the random initialization method.}
}